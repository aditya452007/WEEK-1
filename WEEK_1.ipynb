{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2030f12e",
   "metadata": {},
   "source": [
    "## Cell 1: Importing Required Libraries\n",
    "\n",
    "**Description:**  \n",
    "This cell imports the essential libraries, `pandas` for data manipulation and `numpy` for numerical operations.\n",
    "\n",
    "**Improvements:**  \n",
    "- Ensures all subsequent cells can use `pd` and `np` without repeated imports.\n",
    "- Sets up the environment for efficient data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d00519",
   "metadata": {},
   "source": [
    "## Cell 2: Reading the Excel Data\n",
    "\n",
    "**Description:**  \n",
    "Defines the file path and sheet name, then reads the Excel file into a DataFrame named `data_orig`.\n",
    "\n",
    "**Improvements:**  \n",
    "- Uses raw string for file path to avoid escape character issues.\n",
    "- Reads only the specified sheet, making the process efficient and targeted.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the file name and the data sheet\n",
    "orig_data_file = r\"climate_change_download_0.xls\"\n",
    "data_sheet = \"Data\"\n",
    "\n",
    "# read the data from the excel file to a pandas DataFrame\n",
    "data_orig = pd.read_excel(io=orig_data_file, sheet_name=data_sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a87cc2",
   "metadata": {},
   "source": [
    "## Cell 3: Checking Dataset Shape\n",
    "\n",
    "**Description:**  \n",
    "Prints the shape (rows, columns) of the original dataset.\n",
    "\n",
    "**Improvements:**  \n",
    "- Provides a quick overview of dataset size for initial assessment.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3016984",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the original dataset:\")\n",
    "data_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8126626",
   "metadata": {},
   "source": [
    "## Cell 4: Displaying Column Names\n",
    "\n",
    "**Description:**  \n",
    "Prints all available columns in the dataset.\n",
    "\n",
    "**Improvements:**  \n",
    "- Helps in understanding the structure and available features for further processing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855900eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available columns:\")\n",
    "data_orig.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2f3408",
   "metadata": {},
   "source": [
    "## Cell 5: Displaying Column Data Types\n",
    "\n",
    "**Description:**  \n",
    "Prints the data types of each column.\n",
    "\n",
    "**Improvements:**  \n",
    "- Identifies which columns are numeric, categorical, or object types for appropriate cleaning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7025226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column data types:\")\n",
    "data_orig.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428974dd",
   "metadata": {},
   "source": [
    "## Cell 6: Previewing the Data\n",
    "\n",
    "**Description:**  \n",
    "Displays the first five rows of the dataset.\n",
    "\n",
    "**Improvements:**  \n",
    "- Offers a snapshot of the data for visual inspection and validation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overview of the first 5 rows:\")\n",
    "data_orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb927a85",
   "metadata": {},
   "source": [
    "## Cell 7: Descriptive Statistics\n",
    "\n",
    "**Description:**  \n",
    "Prints descriptive statistics for the columns.\n",
    "\n",
    "**Improvements:**  \n",
    "- Summarizes key statistics (count,unique,top,freq) to understand data distribution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20985af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Descriptive statistics of the columns:\")\n",
    "data_orig.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5656c",
   "metadata": {},
   "source": [
    "## Cell 8: Unique Series Names\n",
    "\n",
    "**Description:**  \n",
    "Displays unique values in the 'Series name' column.\n",
    "\n",
    "**Improvements:**  \n",
    "- Helps identify all available indicators/features in the dataset.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7286f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig['Series name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b4df8",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 9: Unique Series Codes\n",
    "\n",
    "**Description:**  \n",
    "Displays unique values in the 'Series code' column.\n",
    "\n",
    "**Improvements:**  \n",
    "- Useful for mapping or filtering specific series if needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig['Series code'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621a8a9",
   "metadata": {},
   "source": [
    "## Cell 10: Unique SCALE Values\n",
    "\n",
    "**Description:**  \n",
    "Displays unique values in the 'SCALE' column.\n",
    "\n",
    "**Improvements:**  \n",
    "- Identifies non-numeric or problematic scale entries for cleaning.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3d546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig['SCALE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b824ae",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 11: Unique Decimals Values\n",
    "\n",
    "**Description:**  \n",
    "Displays unique values in the 'Decimals' column.\n",
    "\n",
    "**Improvements:**  \n",
    "- Detects non-numeric entries that may require cleaning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26911dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig['Decimals'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f679d073",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 12: Rows with 'Text' in SCALE\n",
    "\n",
    "**Description:**  \n",
    "Prints rows where the 'SCALE' column contains 'Text'.\n",
    "\n",
    "**Improvements:**  \n",
    "- Pinpoints rows with non-numeric scale values for targeted removal.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d46a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rows where SCALE is 'Text'\n",
    "print(\"Rows with 'Text' in SCALE column:\")\n",
    "data_orig[data_orig['SCALE'] == 'Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9873ef32",
   "metadata": {},
   "source": [
    "## Cell 13: Rows with 'Text' in Decimals\n",
    "\n",
    "**Description:**  \n",
    "Prints rows where the 'Decimals' column contains 'Text'.\n",
    "\n",
    "**Improvements:**  \n",
    "- Identifies rows with non-numeric decimals for cleaning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc92c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rows where Decimals is 'Text'\n",
    "print(\"Rows with 'Text' in Decimals column:\")\n",
    "data_orig[data_orig['Decimals'] == 'Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6843b4",
   "metadata": {},
   "source": [
    "## Cell 14: Removing Non-Numeric SCALE Rows\n",
    "\n",
    "**Description:**  \n",
    "Creates a deep copy of the original data, then removes rows where 'SCALE' is 'Text'.\n",
    "\n",
    "**Improvements:**  \n",
    "- Ensures only numeric data is retained for analysis.\n",
    "- Prevents accidental modification of the original dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5691a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the data to a new DataFrame (create a deep copy to avoid modifying the original)\n",
    "data_clean = data_orig.copy()\n",
    "\n",
    "print(\"Original number of rows:\")\n",
    "print(data_clean.shape[0])\n",
    "\n",
    "# remove rows where the 'SCALE' column is 'Text'\n",
    "data_clean = data_clean[data_clean['SCALE'] != 'Text']\n",
    "\n",
    "print(\"Number of rows after removing 'Text' in 'SCALE':\")\n",
    "print(data_clean.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2516d55",
   "metadata": {},
   "source": [
    "## Cell 15: Dropping Metadata Columns\n",
    "\n",
    "**Description:**  \n",
    "Drops unnecessary metadata columns if they exist.\n",
    "\n",
    "**Improvements:**  \n",
    "- Reduces dataset complexity by removing irrelevant columns.\n",
    "- Checks for column existence before dropping to avoid errors.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed8cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original number of columns:\")\n",
    "print(data_clean.shape[1])\n",
    "\n",
    "# Columns to drop\n",
    "cols_to_drop = ['Country name', 'Series code', 'SCALE', 'Decimals']\n",
    "\n",
    "# Drop only if they exist in the DataFrame\n",
    "data_clean = data_clean.drop(columns=[col for col in cols_to_drop if col in data_clean.columns])\n",
    "\n",
    "print(\"Number of columns after dropping metadata columns:\")\n",
    "print(data_clean.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef47148",
   "metadata": {},
   "source": [
    "## Cell 16: Replacing Missing Value Placeholders\n",
    "\n",
    "**Description:**  \n",
    "Replaces missing value placeholders ('' and '..') with `np.nan` in all relevant columns and infers correct data types.\n",
    "\n",
    "**Improvements:**  \n",
    "- Standardizes missing values for consistent handling.\n",
    "- Ensures columns are of appropriate data types for analysis.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing value placeholders with np.nan in all numeric columns (except metadata)\n",
    "cols_to_clean = data_clean.columns[2:]\n",
    "data_clean[cols_to_clean] = data_clean[cols_to_clean].replace({'': np.nan, '..': np.nan})\n",
    "data_clean[cols_to_clean] = data_clean[cols_to_clean].infer_objects(copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c885acf",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 17: Converting Columns to Numeric\n",
    "\n",
    "**Description:**  \n",
    "Creates a copy of the cleaned data and converts all relevant columns to numeric, coercing errors to NaN.\n",
    "\n",
    "**Improvements:**  \n",
    "- Ensures all data used for analysis is numeric.\n",
    "- Handles conversion errors gracefully and reports issues.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d434bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original cleaned data\n",
    "data_clean2 = data_clean.copy()\n",
    "\n",
    "# Select only the numeric columns (from 3rd column onwards)\n",
    "numeric_cols = data_clean2.columns[2:]\n",
    "\n",
    "# Safely convert those columns to numeric, forcing invalid values to NaN\n",
    "for col in numeric_cols:\n",
    "    try:\n",
    "        data_clean2[col] = pd.to_numeric(data_clean2[col], errors='coerce')\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting column {col}: {e}\")\n",
    "\n",
    "# Print data types to verify\n",
    "print(\"Column data types after numeric conversion:\")\n",
    "print(data_clean2.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9a924c",
   "metadata": {},
   "source": [
    "## Cell 18: Renaming Series Names\n",
    "\n",
    "**Description:**  \n",
    "Defines a mapping from long variable names to shorter, more readable names and applies it to the 'Series name' column.\n",
    "\n",
    "**Improvements:**  \n",
    "- Improves readability and usability of the dataset.\n",
    "- Facilitates easier referencing of variables in later analysis.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237545c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define shorter names corresponding to most relevant variables in a dictionary\n",
    "chosen_vars = {'Cereal yield (kg per hectare)': 'cereal_yield',\n",
    "               'Foreign direct investment, net inflows (% of GDP)': 'fdi_perc_gdp',\n",
    "               'Access to electricity (% of total population)': 'elec_access_perc',\n",
    "               'Energy use per units of GDP (kg oil eq./$1,000 of 2005 PPP $)': 'en_per_gdp',\n",
    "               'Energy use per capita (kilograms of oil equivalent)': 'en_per_cap',\n",
    "               'CO2 emissions, total (KtCO2)': 'co2_ttl',\n",
    "               'CO2 emissions per capita (metric tons)': 'co2_per_cap',\n",
    "               'CO2 emissions per units of GDP (kg/$1,000 of 2005 PPP $)': 'co2_per_gdp',\n",
    "               'Other GHG emissions, total (KtCO2e)': 'other_ghg_ttl',\n",
    "               'Methane (CH4) emissions, total (KtCO2e)': 'ch4_ttl',\n",
    "               'Nitrous oxide (N2O) emissions, total (KtCO2e)': 'n2o_ttl',\n",
    "               'Droughts, floods, extreme temps (% pop. avg. 1990-2009)': 'nat_emerg',\n",
    "               'Population in urban agglomerations >1million (%)': 'pop_urb_aggl_perc',\n",
    "               'Nationally terrestrial protected areas (% of total land area)': 'prot_area_perc',\n",
    "               'GDP ($)': 'gdp',\n",
    "               'GNI per capita (Atlas $)': 'gni_per_cap',\n",
    "               'Under-five mortality rate (per 1,000)': 'under_5_mort_rate',\n",
    "               'Population growth (annual %)': 'pop_growth_perc',\n",
    "               'Population': 'pop',\n",
    "               'Urban population growth (annual %)': 'urb_pop_growth_perc',\n",
    "               'Urban population': 'urb_pop'\n",
    "                }\n",
    "\n",
    "# rename all variables in the column \"Series name\" with comprehensible shorter versions\n",
    "data_clean2['Series name'] = data_clean2['Series name'].replace(to_replace=chosen_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836614b9",
   "metadata": {},
   "source": [
    "## Cell 19: Previewing Renamed Data\n",
    "\n",
    "**Description:**  \n",
    "Displays the first five rows of the renamed data.\n",
    "\n",
    "**Improvements:**  \n",
    "- Confirms that renaming was successful and data integrity is maintained.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0381f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1eeb59",
   "metadata": {},
   "source": [
    "## Cell 20: Melting and Merging Features\n",
    "\n",
    "**Description:**  \n",
    "Transforms each chosen feature into a long format and merges all features into a single DataFrame (`all_vars`).\n",
    "\n",
    "**Improvements:**  \n",
    "- Reshapes data for easier analysis and modeling.\n",
    "- Consolidates all relevant features into one unified table.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the short feature names into a list of strings\n",
    "chosen_cols = list(chosen_vars.values())\n",
    "\n",
    "# define an empty list, where sub-dataframes for each feature will be saved\n",
    "frame_list = []\n",
    "\n",
    "# iterate over all chosen features\n",
    "for variable in chosen_cols:\n",
    "\n",
    "    # pick only rows corresponding to the current feature\n",
    "    frame = data_clean2[data_clean2['Series name'] == variable]\n",
    "\n",
    "    # melt all the values for all years into one column and rename the columns correspondingly\n",
    "    frame = frame.melt(id_vars=['Country code', 'Series name']).rename(columns={'Country code': 'country', 'variable': 'year', 'value': variable}).drop(['Series name'], axis='columns')\n",
    "\n",
    "    # add the melted dataframe for the current feature into the list\n",
    "    frame_list.append(frame)\n",
    "\n",
    "\n",
    "# merge all sub-frames into a single dataframe, making an outer binding on the key columns 'country','year'\n",
    "from functools import reduce\n",
    "all_vars = reduce(lambda left, right: pd.merge(left, right, on=['country','year'], how='outer'), frame_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b855a168",
   "metadata": {},
   "source": [
    "## Cell 21: Previewing Merged Data\n",
    "\n",
    "**Description:**  \n",
    "Displays the first five rows of the merged DataFrame.\n",
    "\n",
    "**Improvements:**  \n",
    "- Verifies successful merging and correct data structure.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624086cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7423ccce",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 22: Checking Missing Values per Column\n",
    "\n",
    "**Description:**  \n",
    "Prints the number of missing values in each column.\n",
    "\n",
    "**Improvements:**  \n",
    "- Identifies columns with significant missing data for further cleaning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676020bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"check the amount of missing values in each column\")\n",
    "all_vars.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772008a",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 23: Analyzing Missing Values by Year\n",
    "\n",
    "**Description:**  \n",
    "Counts and prints missing values for each year, sorted by the number of missing entries.\n",
    "\n",
    "**Improvements:**  \n",
    "- Helps decide which years to keep or remove based on data completeness.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbca2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars_clean = all_vars\n",
    "\n",
    "#define an array with the unique year values\n",
    "years_count_missing = dict.fromkeys(all_vars_clean['year'].unique(), 0)\n",
    "for ind, row in all_vars_clean.iterrows():\n",
    "    years_count_missing[row['year']] += row.isnull().sum()\n",
    "\n",
    "# sort the years by missing values\n",
    "years_missing_sorted = dict(sorted(years_count_missing.items(), key=lambda item: item[1]))\n",
    "\n",
    "# print the missing values for each year\n",
    "print(\"missing values by year:\")\n",
    "for key, val in years_missing_sorted.items():\n",
    "    print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40e7765",
   "metadata": {},
   "source": [
    "## Cell 24: Filtering Years with Sufficient Data\n",
    "\n",
    "**Description:**  \n",
    "Filters the dataset to include only years between 1991 and 2008, then prints missing value statistics before and after filtering.\n",
    "\n",
    "**Improvements:**  \n",
    "- Focuses analysis on years with more complete data.\n",
    "- Reduces noise from years with excessive missing values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68704884",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of missing values in the whole dataset before filtering the years:\")\n",
    "print(all_vars_clean.isnull().sum().sum())\n",
    "print(\"number of rows before filtering the years:\")\n",
    "print(all_vars_clean.shape[0])\n",
    "\n",
    "all_vars_clean = all_vars_clean[(all_vars_clean['year'] >= 1991) & (all_vars_clean['year'] <= 2008)]\n",
    "\n",
    "print(\"number of missing values in the whole dataset after filtering the years:\")\n",
    "print(all_vars_clean.isnull().sum().sum())\n",
    "print(\"number of rows after filtering the years:\")\n",
    "print(all_vars_clean.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f44e3a",
   "metadata": {},
   "source": [
    "## Cell 25: Analyzing Missing Values by Country\n",
    "\n",
    "**Description:**  \n",
    "Counts and prints missing values for each country, sorted by the number of missing entries.\n",
    "\n",
    "**Improvements:**  \n",
    "- Identifies countries with insufficient data for potential removal.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43797491",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_count_missing = dict.fromkeys(all_vars_clean['country'].unique(), 0)\n",
    "\n",
    "for ind, row in all_vars_clean.iterrows():\n",
    "    countries_count_missing[row['country']] += row.isnull().sum()\n",
    "\n",
    "countries_missing_sorted = dict(sorted(countries_count_missing.items(), key=lambda item: item[1]))\n",
    "\n",
    "print(\"missing values by country:\")\n",
    "for key, val in countries_missing_sorted.items():\n",
    "    print(key, \":\", val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326f7dbc",
   "metadata": {},
   "source": [
    "## Cell 26: Filtering Countries with Sufficient Data\n",
    "\n",
    "**Description:**  \n",
    "Filters the dataset to include only countries with fewer than 90 missing values, then prints missing value statistics before and after filtering.\n",
    "\n",
    "**Improvements:**  \n",
    "- Ensures only countries with adequate data are retained for analysis.\n",
    "- Improves data quality for downstream tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of missing values in the whole dataset before filtering the countries:\")\n",
    "print(all_vars_clean.isnull().sum().sum())\n",
    "print(\"number of rows before filtering the countries:\")\n",
    "print(all_vars_clean.shape[0])\n",
    "\n",
    "\n",
    "# filter only rows for countries with less than 90 missing values\n",
    "countries_filter = []\n",
    "for key, val in countries_missing_sorted.items():\n",
    "    if val<90:\n",
    "        countries_filter.append(key)\n",
    "\n",
    "all_vars_clean = all_vars_clean[all_vars_clean['country'].isin(countries_filter)]\n",
    "\n",
    "print(\"number of missing values in the whole dataset after filtering the countries:\")\n",
    "print(all_vars_clean.isnull().sum().sum())\n",
    "print(\"number of rows after filtering the countries:\")\n",
    "print(all_vars_clean.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9ab86",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 27: Final Missing Value Check\n",
    "\n",
    "**Description:**  \n",
    "Prints the number of missing values in each column after filtering.\n",
    "\n",
    "**Improvements:**  \n",
    "- Confirms the effectiveness of previous cleaning steps.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec43be66",
   "metadata": {},
   "source": [
    "## Cell 28: Dropping Columns with Excessive Missing Values\n",
    "\n",
    "**Description:**  \n",
    "Drops columns with more than 20 missing values and prints the remaining missing values per column.\n",
    "\n",
    "**Improvements:**  \n",
    "- Further improves data quality by removing problematic features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a07a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for columns with more than 20 missing values\n",
    "vars_bad = all_vars_clean.isnull().sum() > 20\n",
    "\n",
    "# Drop columns where vars_bad is True (columns with > 20 missing values)\n",
    "all_vars_clean2 = all_vars_clean.drop(columns=vars_bad[vars_bad].index)\n",
    "\n",
    "print(\"Remaining missing values per column:\")\n",
    "print(all_vars_clean2.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12209a19",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 29: Dropping Rows with Any Missing Values\n",
    "\n",
    "**Description:**  \n",
    "Removes all rows with any missing values, prints the final missing value counts, dataset shape, and number of rows dropped.\n",
    "\n",
    "**Improvements:**  \n",
    "- Produces a fully complete dataset for robust analysis.\n",
    "- Provides transparency on data loss due to cleaning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf1d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete rows with any number of missing values\n",
    "all_vars_clean3 = all_vars_clean2.dropna(axis='rows', how='any')\n",
    "\n",
    "print(\"Remaining missing values per column:\")\n",
    "print(all_vars_clean3.isnull().sum())\n",
    "\n",
    "print(\"Final shape of the cleaned dataset:\")\n",
    "print(all_vars_clean3.shape)\n",
    "print(f\"Number of rows dropped: {all_vars_clean2.shape[0] - all_vars_clean3.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667ebfc2",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 30: Exporting the Cleaned Data (Commented)\n",
    "\n",
    "**Description:**  \n",
    "Provides code (commented out) to export the final cleaned DataFrame to a CSV file.\n",
    "\n",
    "**Improvements:**  \n",
    "- Facilitates easy saving and sharing of the cleaned dataset.\n",
    "- Keeps export optional to avoid accidental overwrites.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c874458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export the clean dataframe to a csv file\n",
    "all_vars_clean3.to_csv('data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4f4a99",
   "metadata": {},
   "source": [
    "## Summary of Improvements and Further Recommendations\n",
    "\n",
    "**Summary of Improvements:**\n",
    "\n",
    "- **Clear Documentation:** Each cell now includes a markdown description, making the workflow transparent and easy to follow.\n",
    "- **Efficient Imports:** Libraries are imported only once, reducing redundancy and improving notebook performance.\n",
    "- **Stepwise Data Cleaning:** The data cleaning process is broken down into logical, well-documented steps, ensuring clarity and reproducibility.\n",
    "- **Targeted Filtering:** Non-numeric and irrelevant data are systematically removed, improving data quality for analysis.\n",
    "- **Consistent Handling of Missing Values:** Missing values are standardized and handled in multiple stages, resulting in a robust, clean dataset.\n",
    "- **Feature Renaming:** Long variable names are replaced with concise, readable names, enhancing usability and code readability.\n",
    "- **Data Reshaping and Merging:** The dataset is transformed into a long format and merged efficiently, facilitating advanced analysis.\n",
    "- **Progressive Filtering:** Years and countries with insufficient data are filtered out, ensuring the final dataset is both comprehensive and reliable.\n",
    "- **Transparency:** Each cleaning step reports the impact on data shape and missing values, providing full transparency.\n",
    "\n",
    "**Further Recommendations for Improvement:**\n",
    "\n",
    "- **Automated Data Validation:** Implement automated checks for data consistency and outlier detection.\n",
    "- **Visualization:** Add exploratory data analysis (EDA) plots to visualize distributions and relationships between variables.\n",
    "- **Parameterization:** Allow filtering thresholds (e.g., missing value limits) to be set as parameters for flexibility.\n",
    "- **Pipeline Modularization:** Refactor the workflow into reusable functions or a pipeline for easier maintenance and scalability.\n",
    "- **Version Control:** Integrate with version control (e.g., Git) to track changes and collaborate efficiently.\n",
    "- **Performance Optimization:** For large datasets, consider using chunked processing or Dask for scalability.\n",
    "- **Unit Testing:** Add assertions or tests to validate each cleaning step, ensuring data integrity throughout the workflow.\n",
    "\n",
    "These improvements make the notebook more robust, maintainable, and user-friendly, while further enhancements can streamline analysis and support more advanced data science workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
